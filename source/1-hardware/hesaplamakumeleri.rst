.. _hesaplama-kumeleri:

==================
Hesaplama KÃ¼meleri
==================

TRUBA hesaplama kÃ¼meleri, her yÄ±l geliÅŸtirilerek gÃ¼ncellenmektedir. YapÄ±lan her gÃ¼ncelleme, zamanÄ±n ihtiyaÃ§larÄ± ve mevcut sunucu teknolojileri gÃ¶zÃ¼ne alÄ±narak yapÄ±ldÄ±ÄŸÄ±ndan, alÄ±nan hesaplama sunucularÄ±nÄ±n modelleri, iÅŸlemcileri, Ã§ekirdek sayÄ±larÄ± ve bellek miktarÄ± farklÄ±lÄ±k gÃ¶stermektedir.
	
Bu kÄ±lavuz aÅŸaÄŸÄ±dakileri kapsamaktadÄ±r:

.. grid:: 3

    .. grid-item-card::  :ref:`truba-kaynaklari`
        :text-align: center
    .. grid-item-card:: :ref:`guncel-sunucu-aileleri`
        :text-align: center
    .. grid-item-card:: :ref:`partitions`
        :text-align: center

.. _truba-kaynaklari:

----------------
TRUBA KaynaklarÄ±
----------------

2003 yÄ±lÄ±nda faaliyete geÃ§en TÃœBÄ°TAK ULAKBÄ°M YÃ¼ksek BaÅŸarÄ±mlÄ± ve Grid Hesaplama Merkezi'nde bulunan kaynaklar TRUBA'ya dahildir. GÃ¼nÃ¼mÃ¼zde TRUBA > 80.000 iÅŸlemci Ã§ekirdeÄŸi, 312 adet GPU ve toplamda 7.5PByte yapÄ±landÄ±rÄ±lmÄ±ÅŸ yÃ¼ksek performanslÄ± Lustre ve WEKA dosya sistemi ile araÅŸtÄ±rmacÄ±larÄ±mÄ±za hizmet vermektedir. 


.. list-table:: TRUBA KaynaklarÄ±
   :widths: 25 25 25 25 25 25 25 25
   :header-rows: 1

   * - YÄ±l
     - Adet
     - CPU/GPU
     - Ä°ÅŸlemci Modeli
     - SPECfp_rate_base2006
     - Teorik Gflops
     - Bellek
     - TanÄ±mÄ± 
   * - 2016
     - 1
     - 14 Ã§ekirdek x 16 CPU
     - Xeon E7-4850 v3 2.20GHz
     - *-*
     - 7.9 Tflops
     - 4 TB 
     - orkinos
   * - 2017
     - 128 
     - 14 Ã§ekirdek x 2 CPU 
     - Xeon E5-2690 v3 2.60GHz
     - 970 
     - 1164 Gflops
     - 256 GB 
     - sardalya
   * - 2018
     - 120
     - 20 Ã§ekirdek x 2 CPU
     - Xeon 6148 2.40GHz
     - 1400
     - 2048Gflops
     - 384 GB
     - barbun
   * - 2018
     - 24
     - 20 Ã§ekirdek x 2CPU & 2xNvidia P100 GPU
     - Xeon 6148 2.40GHz
     - 1400
     - 2048Gflops & 9400Gflops 
     - 384 GB & 2x16 GB HBM2
     - barbun-cuda
   * - 2018
     - 24
     - 20 Ã§ekirdek x 2CPU & 4xNvidia V100 GPU
     - Xeon 6148 2.40GHz
     - 1400 
     - 2048Gflops & 4x7800Gflops
     - 384 GB & 4x16 GB HBM 
     - akya-cuda
   * - 2021
     - 144
     - 28 Ã§ekirdek x 2 CPU
     - Xeon 6258R 2.70GHz
     - *-*
     - 3.234 Tflops
     - 192 GB 
     - hamsi
   * - 2021
     - 9
     - 64 Ã§ekirdek x 2CPU & 8xNvidia A100 GPU
     - AMD 7742 2.24GHz
     - *-*
     - 4.600Gflops & 8x9.7Tflops
     - 1 TB & 8x80GB HBM
     - palamut-cuda
   * - 2023
     - 504
     - 56 Ã§ekirdek x 2CPU 
     - Intel(R) Xeon(R) Platinum 8480+ 2.0GHz
     - *-*
     - 7 Tflops
     - 256 GB
     - orfoz
   * - 2024
     - 24
     - 32 Ã§ekirdek x 2CPU & 4xNvidia H100 GPU
     - Intel(R) Xeon(R) GOLD 6548Y+ 2.5GHz
     - *-*
     - 5Tflops & 4x34Tflops
     - 1 TB & 4x80GB HBM
     - kolyoz-cuda 

.. _guncel-sunucu-aileleri:

----------------------
GÃ¼ncel Sunucu Aileleri
----------------------
GÃ¼ncel olarak kullanÄ±mda olan hesaplama kaynaklarÄ± aÅŸaÄŸÄ±daki gibidir. 

*Orkinos*
^^^^^^^^^
Sunucu Ã¼zerinde 4128 GB bellek, 224 adet Intel Xeon e7-4850 V4 Ã§ekirdeÄŸi bulunmaktadÄ±r. YÃ¼ksek belleki bir SMP sunucusudur. Ãœzerinde Redhat 7.2 iÅŸletim sistemi bulunmaktadÄ±r. Ortak dosya sistemine Infiniband aÄŸ katmanÄ± ile baÄŸlÄ±dÄ±r. Ortak dosya sistemi Ã¼zerindeki uygulamalarÄ±n bÃ¼yÃ¼k bir Ã§oÄŸunluÄŸu bu sistem Ã¼zerinde Ã§alÄ±ÅŸabilir durumdadÄ±r. Ancak uygulamalarÄ±n yÃ¼ksek verimde Ã§alÄ±ÅŸmasÄ± iÃ§in Intel derleyiciler ve MKL kÃ¼tÃ¼phanesi ile ya da GCC derleyicileri ve kÃ¼tÃ¼phaneleri ile derlenirken V3 iÅŸlemcilerin vektÃ¶r komut setlerinin (AVX2) kullanÄ±lmasÄ± iÃ§in Ã¶zellikle parametre girilmesi gerekmektedir.

..
  *Sardalya*
  ^^^^^^^^^^

  Sardalya sunucularÄ± 153 adet Huawei Tecal RH1288 V3 model sunuculardan oluÅŸmaktadÄ±r. Her bir sunucu Ã¼zerinde 2 adet Intel Xeon E5-2690 V4 iÅŸlemci ve toplam 28 adet iÅŸlemci Ã§ekirdeÄŸi bulunmaktadÄ±r. Sunucular birbirlerine EDR (100Gbps) Infiniband aÄŸ kartlarÄ± ile non-blocking yapÄ±da baÄŸlÄ±dÄ±rlar. 

.. _barbun-cuda:

*Barbun ve Barbun-cuda*
^^^^^^^^^^^^^^^^^^^^^^^

Barbun sunucularÄ± 120 adet Dell R640, Barbun-cuda sunucularÄ± R740 model sunuculardan oluÅŸmaktadÄ±r. Her bir sunucu Ã¼zerinde 2 adet Intel Xeon Scalable Gold 6148 iÅŸlemci ve toplam 40 adet iÅŸlemci Ã§ekirdeÄŸi bulunmaktadÄ±r. Sunucular birbirlerine EDR (100Gbps) Infiniband aÄŸ kartlarÄ± ile non-blocking yapÄ±da baÄŸlÄ±dÄ±rlar. Barbun-cuda sunucularÄ±nÄ±n her birinde ``2 adet NVIDIA P100`` GPU kartÄ± bulunmaktadÄ±r.

..
  
  Mevcut durumda barbun bÃ¶lÃ¼mlendirmesinde 3 kuyruk tanÄ±mÄ± bulunmaktadÄ±r: *mid2, long* ve *barbun*. Ã–ncelik sÄ±ralamasÄ± da yÃ¼ksekten dÃ¼ÅŸÃ¼ÄŸe dogru *mid2, long* ve *barbun* ÅŸeklindedir. Ä°ÅŸlerinizin tahmini Ã§alÄ±ÅŸma sÃ¼resine gÃ¶re SLURM betik dosyanÄ±zda mid2 (maksimum 8 gÃ¼n) veya long (maksimum 15 gÃ¼n) kuyruklarÄ±nÄ± tanÄ±mlayabilirsiniz.

.. note::

  barbun-cuda sunucularÄ±nda NVIDIA 565.57.01 sÃ¼rÃ¼cÃ¼sÃ¼ bulunmaktadÄ±r:

  - NVIDIA-SMI 565.57.01              
  - Driver Version: 565.57.01      
  - CUDA Version: 12.7
  
  `NVIDIA 565.57.01 sÃ¼rÃ¼cÃ¼sÃ¼ CUDA 12.x versiyonu ile uyumludur <https://docs.nvidia.com/deploy/cuda-compatibility/>`_ .

.. _akya-cuda:

*Akya-cuda*
^^^^^^^^^^^
Akya-cuda sunucularÄ± 24 adet Supermicro 1029GQ-TRT model sunuculardan oluÅŸmaktadÄ±r. Her bir sunucu Ã¼zerinde 2 adet Intel Xeon Scalable Gold 6148 iÅŸlemci ve toplam 40 adet iÅŸlemci Ã§ekirdeÄŸi ve ``4 adet NVIDIA Tesla V100`` (16GB, NVLink) GPU kartÄ± bulunmaktadÄ±r. Sunucular birbirlerine EDR (100Gbps) Infiniband aÄŸ kartlarÄ± ile non-blocking yapÄ±da baÄŸlÄ±dÄ±rlar.

.. note::

  akya-cuda sunucularÄ±nda NVIDIA 550.90.07 sÃ¼rÃ¼cÃ¼sÃ¼ bulunmaktadÄ±r:

  - NVIDIA-SMI 565.57.01              
  - Driver Version: 565.57.01      
  - CUDA Version: 12.7

  `NVIDIA 565.57.01 sÃ¼rÃ¼cÃ¼sÃ¼ CUDA 12.x versiyonu ile uyumludur <https://docs.nvidia.com/deploy/cuda-compatibility/>`_ .

.. _hamsi:

*Hamsi*
^^^^^^^^^^^^^^^^^^^^
Hamsi sunucularÄ± 144 adet INSPUR NF5180M5 sunuculardan oluÅŸmaktadÄ±r. Her bir sunucu Ã¼zerinde 2 adet Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz iÅŸlemci ve toplam 56 adet iÅŸlemci Ã§ekirdeÄŸi bulunmaktadÄ±r. Sunucular birbirlerine HDR100 (100Gbps) Infiniband aÄŸ kartlarÄ± ile non-blocking yapÄ±da baÄŸlÄ±dÄ±rlar. 

.. _orfoz:

*Orfoz*
^^^^^^^^^^^^^^^^^^^^
Orfoz sunucularÄ± 504 adet Lenovo ThinkSystem SR630 V3 sunuculardan oluÅŸmaktadÄ±r. Her bir sunucu Ã¼zerinde 2 adet Intel(R) Xeon(R) Platinum 8480+ CPU @ 2.0GHz iÅŸlemci ve toplam 112 adet iÅŸlemci Ã§ekirdeÄŸi bulunmaktadÄ±r. Sunucular birbirlerine 200Gbps Infiniband aÄŸ kartlarÄ± ile baÄŸlÄ±dÄ±rlar. 

Orfoz sunucularÄ±nÄ±n yer aldÄ±ÄŸÄ± ARF hesaplama kÃ¼mesi hakkÄ±ndaki ayrÄ±ntÄ±lÄ± bilgilere  :ref:`arf-genel-bilgileri` sayfasÄ±ndan eriÅŸim saÄŸlayabilirsiniz.

.. _kolyoz-cuda:

*Kolyoz-cuda*
^^^^^^^^^^^^^^^^^^^^
Kolyoz sunucularÄ± 24 adet `Lenovo ThinkSystem SD650-N V3 <https://www.lenovo.com/us/en/p/servers-storage/servers/supercomputing/thinksystem-sd650-n-v3-high-density-server/len21ts0028>`_ sunuculardan oluÅŸmaktadÄ±r. Her bir sunucu Ã¼zerinde 2 adet Intel(R) Xeon(R) GOLD 6548Y+ CPU @ 2.5GHz iÅŸlemci ve toplam 64 adet iÅŸlemci Ã§ekirdeÄŸi ve 4 adet NVIDIA H100 (80GB, NVLink, HBM3, SXM5) GPU kartÄ± bulunmaktadÄ±r. Sunucular birbirlerine 4 adet 200Gbps Infiniband aÄŸ kartlarÄ± ile baÄŸlÄ±dÄ±rlar. 

.. note::

 ``kolyoz-cuda`` kuyruÄŸunda Rocky Linux (BLue Onyx) 9.4 iÅŸletim sistemi ve hesaplama sunucularÄ± Ã¼zerinde NVIDIA 560.35.03 sÃ¼rÃ¼cÃ¼sÃ¼ bulunmaktadÄ±r. `NVIDIA 560.35.03 sÃ¼rÃ¼cÃ¼sÃ¼ CUDA 12.* versiyonu ile uyumludur <https://docs.nvidia.com/deploy/cuda-compatibility/>`_ .

  - NVIDIA-SMI 565.57.01
  - Driver Version: 565.57.01
  - CUDA Version: 12.7

Kolyoz sunucularÄ±nÄ±n yer aldÄ±ÄŸÄ± ARF ACC hesaplama kÃ¼mesi hakkÄ±ndaki ayrÄ±ntÄ±lÄ± bilgilere  :ref:`arf-acc-genel-bilgiler` sayfasÄ±ndan eriÅŸim saÄŸlayabilirsiniz.

.. _palamut-cuda:

*Palamut-cuda*
^^^^^^^^^^^^^^^^^^^^^^^^^^
Palamut sunucularÄ± 9 adet HP Proliant XL675d Gen10 Plus model sunuculardan oluÅŸmaktadÄ±r. Her bir sunucu Ã¼zerinde 2 adet AMD EPYC 7742 2.24GHz iÅŸlemci ve toplam 128 adet iÅŸlemci Ã§ekirdeÄŸi ve 8 adet Nvidia Tesla A100 (80GB, NVLink ) GPU kartÄ± bulunmaktadÄ±r. Sunucular birbirlerine 4xHDR (200Gbps) Infiniband aÄŸ kartlarÄ± ile non-blocking yapÄ±da baÄŸlÄ±dÄ±rlar.

.. note::

  Palamut-cuda kuyruÄŸunda Rocky Linux (BLue Onyx) 9.4 iÅŸletim sistemi ve NVIDIA 565.57.01 sÃ¼rÃ¼cÃ¼sÃ¼ bulunmaktadÄ±r. `NVIDIA 565.57.01 sÃ¼rÃ¼cÃ¼sÃ¼ CUDA 12.X versiyonu ile uyumludur <https://docs.nvidia.com/deploy/cuda-compatibility/>`_ .

  - NVIDIA-SMI 565.57.01
  - Driver Version: 565.57.01
  - CUDA Version: 12.7

.. note::

  * kolyoz-cuda ve palamut-cuda hesaplama kÃ¼meleri Ã¶zel kÃ¼me olup sadece araÅŸtÄ±rma merkezleri tarafÄ±ndan yÃ¼rÃ¼tÃ¼len alt yapÄ± projeleri (Ä°lgili altyapÄ± projeleri T.C. CumhurbaÅŸkanlÄ±ÄŸÄ± Strateji ve BÃ¼tÃ§e BaÅŸkanlÄ±ÄŸÄ±, Strateji ve BÃ¼tÃ§e BaÅŸkanlÄ±ÄŸÄ± tarafÄ±ndan desteklenen projelerdir) ve de sÃ¶zleÅŸmeli proje araÅŸtÄ±rmacÄ±larÄ± (TÃœBÄ°TAK ULAKBÄ°M ile proje kapsamÄ±nda sÃ¶zleÅŸmesi olan projeler) tarafÄ±ndan kullanÄ±labilmektedir.Â Barbun-cuda ve akya-cuda hesaplama kÃ¼meleri tÃ¼m kullanÄ±cÄ±larÄ±mÄ±zÄ±n eriÅŸimine aÃ§Ä±ktÄ±r.

  * Mevcut durumda kolyoz-cuda ve palamut-cuda kuyruklarÄ± Ã¶ncelikli olarak belirli araÅŸtÄ±rma gruplarÄ±na hizmet vermektedir. Bu araÅŸtÄ±rma gruplarÄ±nda hesaplarÄ± tanÄ±mlÄ± olan kullanÄ±cÄ±lar ``kolyoz-cuda`` ve/veya ``palamut-cuda`` hesaplama kÃ¼mesine iÅŸ gÃ¶nderebileceklerdir.

  * kolyoz-cuda ve palamut-cuda hesaplama kÃ¼meleri iÃ§in ayrÄ± bir kullanÄ±cÄ± arayÃ¼zÃ¼ kurulmuÅŸtur (``cuda-ui``). Ä°lgili cuda kuyruklarÄ±na sadece ``cuda-ui`` arayÃ¼zÃ¼ Ã¼zerinden iÅŸ gÃ¶nderilebilecektir. 

  * cuda-ui kullanÄ±cÄ± arayÃ¼zÃ¼ Ã¼zerinde Rocky Linux 9.2 iÅŸletim sistemi bulunmaktadÄ±r.
  
  * kolyoz-cuda ve palamut-cuda hesaplama kÃ¼melerine eriÅŸim izni olan proje kullanÄ±cÄ±larÄ± aktif bir OpenVPN baÄŸlantÄ±larÄ± mevcut iken ``cuda-ui`` arayÃ¼z sunucusuna ``172.16.6.16`` IP adresine veya ``cuda-ui.yonetim`` adresine ssh ile baÄŸlanabilirler.

  .. code-block:: bash

    ssh username@172.16.6.16
  
  veya

  .. code-block:: bash

    ssh username@cuda-ui.yonetim
  
  Veya aktif bir OpenVPN baÄŸlantÄ±sÄ± ile arf-ui kullanÄ±cÄ± arayÃ¼z sunucularÄ±ndan birisine baÄŸlÄ± iken ilgili kullanÄ±cÄ± arayÃ¼zÃ¼ Ã¼zerinden cuda-ui arayÃ¼z sunucusuna ssh ile geÃ§iÅŸ yapabilirler. SSH anahtalarÄ±nÄ± henÃ¼z oluÅŸturmamÄ±ÅŸ kullanÄ±cÄ±lar, bu sunucuya arf-ui Ã¼zerinden geÃ§iÅŸ yapabilmek iÃ§in ssh anahtarlarÄ±nÄ± ``ssh-keygen`` ile aÅŸaÄŸÄ±daki gibi oluÅŸturabilirler:

  .. code-block::

    $>ssh-keygen (Sorulan tÃ¼m sorularÄ± â€œEnterâ€ tuÅŸuna basarak geÃ§iniz)
   
    $>cp -p .ssh/id_rsa.pub .ssh/authorized_keys

.. warning::

   Her bir GPU icin kullanÄ±cÄ±lar 16 Ã§ekirdek talep etmelidir. Ã–rneÄŸin: 2 sunucu Ã¼zerinde 4'er gÃ¶rev ve 4'er GPU kullanabilmek icin:
	
   .. code-block::

      srun -N 2  -n 8 -c 16 --gres=gpu:4   <komut>
      sbatch  -N 2  -n 8 -c 16 --gres=gpu:4 <betik_dosyasi>

   KullanÄ±lacak toplam Ã§ekirdek sayÄ±sÄ± = n x c

   N : kulllanÄ±lacak node sayÄ±sÄ±

   n : Ã§alÄ±ÅŸtÄ±rÄ±lacak gÃ¶rev sayÄ±sÄ±

   c : her bir gÃ¶rev iÃ§in kullanÄ±lacak Ã§ekirdek sayÄ±sÄ± (varsayÄ±lan 1)

   gres=gpu:x : her bir node Ã¼zerinde kullanÄ±lacak GPU sayÄ±sÄ±

.. _partitions:

----------------------
Kuyruklar (partitions)
----------------------

Her iki kÃ¼mede iÅŸ kuyruÄŸu adlarÄ± ve Ã¶zellikleri aynÄ± ÅŸekilde yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r. 

Zaman zaman bazÄ± kuyruklardaki kaynak miktarÄ± arttÄ±rÄ±labilir ya da azaltÄ±labilir, bazÄ± kuyruklar kullanÄ±mdan kaldÄ±rÄ±labilir. Herhangi bir kuyruÄŸun bilgisine aÅŸaÄŸÄ±daki komutla eriÅŸilebilir: 

.. code-block::

   scontrol show partition=kuyruk_adi 

KuyruklarÄ±n kullanÄ±m durumuna, paylaÅŸÄ±lan, dolu ya da boÅŸ olan node ve Ã§ekirdeklerin durumuna ``sinfo`` komutu ile eriÅŸilebilir. 

TÃ¼m kuyruklarÄ±n varsayÄ±lan Ã§alÄ±ÅŸma sÃ¼resi 2 dakikadÄ±r. Betik dosyasÄ±nda zaman bilgisi girilmeyen iÅŸler 2 dakika sonunda otomatik olarak sonlandÄ±rÄ±lmaktadÄ±r. Slurm betik dosyasÄ±nda `#SBATCH -\-time <https://slurm.schedmd.com/sbatch.html>`_ komutu ile hesaplama iÃ§in Ã¶ngÃ¶rÃ¼len zaman bilgisi girilen iÅŸler, belirtilen zaman sonunda otomatik olarak sonlandÄ±rÄ±lmaktadÄ±r. 

Her sunucu ailesinde, sunucu Ã¼zerindeki Ã§ekirdek sayÄ±sÄ±na ve bellek miktarÄ±na baÄŸlÄ± olarak bellek sÄ±nÄ±rlamalarÄ± mevcuttur. EÄŸer betik dosyalarÄ±nda (ya da srun komutunda) herhangi bir bellek deÄŸeri girilmemiÅŸse, ilgili iÅŸ iÃ§in, ``Ã§ekirdek sayÄ±sÄ± x DefMemPerCore`` kadar bellek ayrÄ±lÄ±r. Betik dosyalarÄ±nda (ya da srun komutunda) iÅŸler iÃ§in ``--mem-per-core`` ya da ``--mem`` parametreleri ile daha fazla bellek talebinde bulunulabilir, ancak talep edilen bellek miktarÄ± hiÃ§bir koÅŸulda *maxMemPerCore* degerini geÃ§emez. *MaxMemPerCore* ve *DefMemPerCore* deÄŸerleri her sunucu ailesi iÃ§in farklÄ±dÄ±r. TÃ¼m sunucular iÃ§in bu verilere aÅŸaÄŸÄ±daki tablodan eriÅŸilebilir. 

.. list-table:: Kuyruklar (partitions)
   :widths: 25 25 25 25 25 25 25 25
   :header-rows: 1
   
   * - YÄ±l
     - Adet
     - CPU/GPU
     - Ä°ÅŸlemci Modeli
     - SPECfp_rate_base2006
     - Teorik Gflops
     - Bellek
     - TanÄ±mÄ±
   * - ğŸ›‘ single
     - levrekv2 
     - 8
     - 3-00:00:00
     - 1
     - 9500MB
     - 10500MB  
     - KullanÄ±m DÄ±ÅŸÄ±
   * - ğŸ›‘ mid2
     - barbun
     - 189
     - 08-00:00:00 
     - 4    
     - 8000MB     
     - 9000MB
     - KullanÄ±m DÄ±ÅŸÄ±
   * - ğŸ›‘ sardalya
     - sardalya
     - 100   
     - 3-00:00:00
     - 4
     - 8000MB
     - 9000MB
     - KullanÄ±m DÄ±ÅŸÄ± 
   * - ğŸ›‘ long
     - barbun
     - 189
     - 3-00:00:00
     - 4  
     - 8000MB, 8500MB   
     - 9000MB, 9500MB 
     - KullanÄ±m DÄ±ÅŸÄ±
   * - ğŸŸ¢ debug
     - orfoz, hamsi, barbun, barbun-cuda, akya-cuda
     - 238
     - 00-04:00:00
     - 1
     - 8000MB 
     - 9500MB
     - Aktif 
   * - ğŸ”‘ smp
     - orkinos
     - 1
     - 3-00:00:00
     - 4 
     - 17000MB  
     - 18400MB
     - Ã–zel Kuyruk
   * - ğŸŸ¢ barbun 
     - barbun   
     - 119  
     - 03-00:00:00  
     - 4
     - 8500MB  
     - 9500MB
     - Aktif
   * - ğŸŸ¢ barbun-cuda 
     - barbun-cuda 
     - 24  
     - 03-00:00:00  
     - 20       
     - 8500MB    
     - 9500MB   
     - Aktif
   * - ğŸŸ¢ akya-cuda
     - akya-cuda
     - 24  
     - 03-00:00:00 
     - 10 
     - 8500MB   
     - 9500MB
     - Aktif
   * - ğŸŸ¢ palamut-cuda
     - palamut
     - 9  
     - 03-00:00:00
     - 16   
     - 7500MB  
     - 16384MB
     - Aktif 
   * - ğŸŸ¢ hamsi  
     - hamsi 
     - 144  
     - 03-00:00:00      
     - 28    
     - 3400MB 
     - 3400MB
     - Aktif
   * - ğŸŸ¢ orfoz 
     - orfoz 
     - 504
     - 03-00:00:00  
     - 56 / 55
     - 2000MB 
     - 2295MB
     - Aktif
   * - ğŸŸ¢ kolyoz-cuda
     - kolyoz
     - 24
     - 03-00:00:00  
     - 16  
     - 16GB 
     - 16GB
     - Aktif



..
  ``mid2`` ve ``long`` kuyruklarÄ±na gÃ¶nderilen iÅŸler sardalya ya da barbun sunucularÄ±nÄ±n herhangi birinde Ã§alÄ±ÅŸmaya baÅŸlayabilirler. Bu kuyruklara gÃ¶nderilecek iÅŸlerin belli bir sunucu ailesi Ã¼zerinde Ã§alÄ±ÅŸmasÄ± isteniyorsa, betik dosyalarÄ±na aÅŸaÄŸÄ±daki tanÄ±mlar yazÄ±lmalÄ±dÄ±r: 

  * barbunlar iÃ§in 

  .. code-block::

   #SBATCH --constraint=barbun 

  * sardalyalar iÃ§in 

  .. code-block::

   #SBATCH --constraint=sardalya 

  .. note::

   --contstraint parametresi yerine -C de kullanÄ±labilir. 

  Ä°ÅŸler Ã¶nceden olduÄŸu gibi Ã¼st kuyruklar yerine doÄŸrudan sardalya, barbun veya diÄŸer kuyruklarÄ±na gÃ¶nderilebilir. 

.. warning::

  *barbun-cuda, akya-cuda*, *palamut-cuda* ve *kolyoz-cuda* kuyruklarÄ±na gÃ¶nderilen iÅŸlerin GPU kullanabilecek ve GPU talep eden iÅŸler olmasÄ± zorunludur.  GPU kÃ¼melerinin kullanÄ±mÄ± ile ilgili dokÃ¼mantasyon :ref:`gpu-kilavuzu` sayfamÄ±zÄ± inceleyebilirsiniz.

..

  *Single*
  ^^^^^^^^^

  Bu kuyruÄŸa tek Ã§ekirdeklik (genelde seri) iÅŸler gÃ¶nderilir. Toplam Ã§ekirdek sayÄ±sÄ± 1'den fazla ise, iÅŸ baÅŸka bir kuyruÄŸa gÃ¶nderilmiÅŸ olsa bile, otomatik olarak bu kuyruÄŸa yÃ¶nlendirilir. 

  Bu kuyruktaki herhangi bir iÅŸin Ã§alÄ±ÅŸma sÃ¼resi en fazla 3 gÃ¼ndÃ¼r. 3 gÃ¼n iÃ§inde tamamlanmamÄ±ÅŸ iÅŸler sistem tarafÄ±ndan otomatik olarak sonlandÄ±rÄ±lmaktadÄ±r. 

  Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi

  .. code-block::

   scontrol show partition=single 

  komutu ile gÃ¶rÃ¼lebilir. 

*Debug*
^^^^^^^^^

Bu kuyruÄŸa test amaÃ§lÄ± kÄ±sa sÃ¼reli iÅŸler (Ã¶reneÄŸin SLURM betik dosyanÄ±zÄ±n, kodunuzun doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olmak iÃ§in) gÃ¶nderilir. Bu kuyruktaki herhangi bir iÅŸin Ã§alÄ±ÅŸma sÃ¼resi en fazla 15 dakikadÄ±r. 3 gÃ¼n iÃ§inde tamamlanmamÄ±ÅŸ iÅŸler sistem tarafÄ±ndan otomatik olarak sonlandÄ±rÄ±lmaktadÄ±r. 

Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi

.. code-block::

   scontrol show partition=debug

komutu ile gÃ¶rÃ¼lebilir. Ä°lgili kuyrukta barbun, barbun-cuda, akya-cuda ve orkinos sunucularÄ± tanÄ±mlÄ±dÄ±r. Bu kuyruklara gÃ¶nderilecek iÅŸlerin belli bir sunucu ailesi Ã¼zerinde Ã§alÄ±ÅŸmasÄ± isteniyorsa, betik dosyalarÄ±na aÅŸaÄŸÄ±daki tanÄ±mlar yazÄ±lmalÄ±dÄ±r: 

* Barbun iÃ§in 

.. code-block::

  #SBATCH --constraint=barbun 

* Barbun-cuda iÃ§in 

.. code-block::

  #SBATCH --constraint=barbun-cuda

* Akya-cuda iÃ§in 

.. code-block::

  #SBATCH --constraint=akya-cuda

* Orkinos iÃ§in 

.. code-block::

  #SBATCH --constraint=smp

.. note::

  `-\-contstraint` parametresi yerine `-C` de kullanÄ±labilir. 


*Mid2*
^^^^^^

Mid2 kuyruÄŸundaki iÅŸlerin Ã§alÄ±ÅŸma sÃ¼resi en fazla 8 gÃ¼ndÃ¼r. 8 gÃ¼n iÃ§erisinde tamamlanmamÄ±ÅŸ iÅŸler sistem tarafÄ±ndan otomatik olarak sonlandÄ±rÄ±lmaktadÄ±r. 

Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi 

.. code-block::

   scontrol show partition=mid2 

komutu ile gÃ¶rÃ¼lebilir. 

*Long*
^^^^^^

Long kuyruÄŸundaki iÅŸlerin Ã§alÄ±ÅŸma sÃ¼resi en fazla 15 gÃ¼ndÃ¼r. Bu sÃ¼re zarfÄ±nda tamamlanmamÄ±ÅŸ iÅŸler sistem tarafÄ±ndan otomatik olarak sonlandÄ±rÄ±lmaktadÄ±r. 

Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi 

.. code-block::

   scontrol show partition=long 

komutu ile gÃ¶rÃ¼lebilir. 

*Interactive*
^^^^^^^^^^^^^

Interaktif iÅŸler Ã§alÄ±ÅŸtÄ±rmak iÃ§in kullanÄ±lÄ±r. Ä°nteraktif iÅŸler ``Ondemand`` Ã¼zerinden ya da SSH terminalinden ``srun``, ``salloc`` ile kuyruÄŸa gÃ¶nderilebilir. Bu kuyrukta levrekv2 sunucularÄ± kullanÄ±lmaktadÄ±r.

*Smp*
^^^^^

*Smp* kuyruÄŸunda sadece *orkinos1* sunucusu bulunmaktadÄ±r. Kuyruk rezervasyon yÃ¶netimi ile Ã§alÄ±ÅŸtÄ±rÄ±lmaktadÄ±r. Bu kuyruÄŸu kullanmak isteyen kullanÄ±cÄ±larÄ±n e-posta ile baÅŸvuruda bulunarak sistemi ne kadar sÃ¼re ile kullanacaklarÄ±nÄ±, ne kadar kaynaÄŸa (iÅŸlemci/bellek) ihtiyaÃ§ duyduklarÄ±nÄ± bildirmeleri ve ihtiyaÃ§larÄ±na gÃ¶re bir rezervasyon yaptÄ±rmalarÄ± gerekmektedir.

Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi

.. code-block::

   scontrol show partition=smp

komutu ile gÃ¶rÃ¼lebilir.

..
  *Sardalya*
  ^^^^^^^^^^

  Her bir sunucuda 28 Ã§ekirdek ve 256GB bellek bulunmaktadÄ±r. Kuyrukta iÅŸlerin en fazla Ã§alÄ±ÅŸma sÃ¼resi 15 gÃ¼ndÃ¼r. Sistemin verimli kullanÄ±labilmesi iÃ§in gÃ¶nderilecek iÅŸler en az 14 Ã§ekirdek talep etmelidir. KuyruÄŸa gÃ¶nderilebilecek iÅŸlerin minimum Ã§ekirdek sayÄ±sÄ± 4'tÃ¼r.

  Ä°ÅŸlerde bellek sÄ±nÄ±rlamasÄ± kullanÄ±lmaktadÄ±r. GÃ¶nderilen iÅŸlerin sunucularÄ±n bellek sÄ±nÄ±rlamalarÄ±na uygun olarak gÃ¶nderilmesi gerekmektedir. Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi

  .. code-block::

    scontrol show partition=sardalya

  komutu ile gÃ¶rÃ¼lebilir.

.. _barbun-node:

*Barbun*
^^^^^^^^

Her bir sunucuda 40 Ã§ekirdek ve 384GB bellek bulunmaktadÄ±r. Kuyrukta iÅŸlerin en fazla Ã§alÄ±ÅŸma sÃ¼resi 15 gÃ¼ndÃ¼r. Sistemin verimli kullanÄ±labilmesi iÃ§in gÃ¶nderilecek iÅŸler en az 20 Ã§ekirdek talep etmelidir. KuyruÄŸa gÃ¶nderilebilecek iÅŸlerin minimum Ã§ekirdek sayÄ±sÄ± 4'tÃ¼r.

Ä°ÅŸlerde bellek sÄ±nÄ±rlamasÄ± kullanÄ±lmaktadÄ±r. GÃ¶nderilen iÅŸlerin sunucularÄ±n bellek sÄ±nÄ±rlamalarÄ±na uygun olarak gÃ¶nderilmesi gerekmektedir. Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi

.. code-block::

   scontrol show partition=barbun

komutu ile gÃ¶rÃ¼lebilir.

.. _barbuncuda-node:

*Barbun-cuda*
^^^^^^^^^^^^^

Her bir sunucuda 40 Ã§ekirdek ve 384GB bellek ayrÄ±ca 2'ÅŸer adet Nvidia P100 16GB GPU kartÄ± bulunmaktadÄ±r. Kuyrukta iÅŸlerin en fazla Ã§alÄ±ÅŸma sÃ¼resi 3 gÃ¼ndÃ¼r. Sistemin verimli kullanÄ±labilmesi iÃ§in gÃ¶nderilecek iÅŸler en az 20 Ã§ekirdek ve 1 GPU talep etmelidir.

*AynÄ± sunucuda Ã§alÄ±ÅŸmaya baÅŸlayan birden fazla iÅŸ aynÄ± GPU kartÄ±nÄ± paylaÅŸabilmektedir.*

Ä°ÅŸlerde bellek sÄ±nÄ±rlamasÄ± kullanÄ±lmaktadÄ±r. GÃ¶nderilen iÅŸlerin sunucularÄ±n bellek sÄ±nÄ±rlamalarÄ±na uygun olarak gÃ¶nderilmesi gerekmektedir. Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi

.. code-block::

   scontrol show partition=barbun-cuda

komutu ile gÃ¶rÃ¼lebilir.

*Akya-cuda*
^^^^^^^^^^^

Her bir sunucuda 40 Ã§ekirdek ve 384GB bellek ayrÄ±ca 4'er adet Nvidia V100 16GB GPU (NVLink) kartÄ± bulunmaktadÄ±r. Kuyrukta iÅŸlerin en fazla Ã§alÄ±ÅŸma sÃ¼resi 3 gÃ¼ndÃ¼r. AyrÄ±ca sistemlerde scratch olarak kullanÄ±lmak Ã¼zere 1.4TB NVME disk /tmp dizinine baÄŸlanmÄ±ÅŸtÄ±r. YÃ¼ksek I/O gerektiren iÅŸlerin /tmp dizininde Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± gerekmektedir.

Ä°ÅŸlerde bellek sÄ±nÄ±rlamasÄ± kullanÄ±lmaktadÄ±r. GÃ¶nderilen iÅŸlerin sunucularÄ±n bellek sÄ±nÄ±rlamalarÄ±na uygun olarak gÃ¶nderilmesi gerekmektedir. Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi

.. code-block::

   scontrol show partition=akya-cuda

komutu ile gÃ¶rÃ¼lebilir.

.. _hamsi-node:

*Hamsi*
^^^^^^^

Her bir sunucuda 56 Ã§ekirdek ve 192GB bellek bulunmaktadÄ±r. Kuyrukta iÅŸlerin en fazla Ã§alÄ±ÅŸma sÃ¼resi 3 gÃ¼ndÃ¼r. Sistemin verimli kullanÄ±labilmesi iÃ§in gÃ¶nderilecek iÅŸler en az 28 Ã§ekirdek talep etmelidir. KuyruÄŸa gÃ¶nderilebilecek iÅŸlerin minimum Ã§ekirdek sayÄ±sÄ± 28'dir.

Ä°ÅŸlerde bellek sÄ±nÄ±rlamasÄ± kullanÄ±lmaktadÄ±r. GÃ¶nderilen iÅŸlerin sunucularÄ±n bellek sÄ±nÄ±rlamalarÄ±na uygun olarak gÃ¶nderilmesi gerekmektedir. Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi

.. code-block::

   scontrol show partition=hamsi

komutu ile gÃ¶rÃ¼lebilir.

.. _orfoz-node:

*orfoz*
^^^^^^^

Her bir sunucuda 112 Ã§ekirdek ve 256GB bellek bulunmaktadÄ±r. Kuyrukta iÅŸlerin en fazla Ã§alÄ±ÅŸma sÃ¼resi 3 gÃ¼ndÃ¼r. Sistemin verimli kullanÄ±labilmesi iÃ§in gÃ¶nderilecek iÅŸler en az 56 Ã§ekirdek talep edilmelidir. KuyruÄŸa gÃ¶nderilebilecek iÅŸlerin minimum Ã§ekirdek sayÄ±sÄ± 56'dÄ±r.

Ä°ÅŸlerde bellek sÄ±nÄ±rlamasÄ± kullanÄ±lmaktadÄ±r. GÃ¶nderilen iÅŸlerin sunucularÄ±n bellek sÄ±nÄ±rlamalarÄ±na uygun olarak gÃ¶nderilmesi gerekmektedir. Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi ilgili kullanÄ±cÄ± arayÃ¼zÃ¼ne baÄŸlandÄ±ktan sonra

.. code-block::

   scontrol show partition=orfoz

komutu ile gÃ¶rÃ¼lebilir.

*Palamut-cuda*
^^^^^^^^^^^^^^

Her bir sunucuda 128 Ã§ekirdek ve 1TB bellek ayrÄ±ca 8'er adet Nvidia A100 80GB GPU (NVLink) kartÄ± bulunmaktadÄ±r. Kuyrukta iÅŸlerin en fazla Ã§alÄ±ÅŸma sÃ¼resi 3 gÃ¼ndÃ¼r. Sistemin verimli kullanÄ±labilmesi iÃ§in gÃ¶nderilecek iÅŸler en az 16 Ã§ekirdek ve 1 GPU talep etmelidir. AyrÄ±ca sistemlerde scratch olarak kullanÄ±lmak Ã¼zere 12TB NVME disk /localscratch dizinine baÄŸlanmÄ±ÅŸtÄ±r. YÃ¼ksek I/O gerektiren iÅŸlerin /localscratch dizininde Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± gerekmektedir.

Ä°ÅŸlerde bellek sÄ±nÄ±rlamasÄ± kullanÄ±lmaktadÄ±r. GÃ¶nderilen iÅŸlerin sunucularÄ±n bellek sÄ±nÄ±rlamalarÄ±na uygun olarak gÃ¶nderilmesi gerekmektedir. Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi palamut-ui kullanÄ±cÄ± arayÃ¼zÃ¼ne baÄŸlandÄ±ktan sonra

.. code-block::

   scontrol show partition=palamut-cuda

komutu ile gÃ¶rÃ¼lebilir.


*Kolyoz-cuda*
^^^^^^^^^^^^^^

Her bir sunucuda 64 Ã§ekirdek ve 1TB bellek ayrÄ±ca 4'er adet NVIDIA H100 80GB GPU (NVLink) kartÄ± bulunmaktadÄ±r. Kuyrukta iÅŸlerin en fazla Ã§alÄ±ÅŸma sÃ¼resi 3 gÃ¼ndÃ¼r. Sistemin verimli kullanÄ±labilmesi iÃ§in gÃ¶nderilecek iÅŸler en az 16 Ã§ekirdek ve 1 GPU talep etmelidir. AyrÄ±ca sistemlerde tmp olarak kullanÄ±lmak Ã¼zere 7TB NVME disk / dizinine baÄŸlanmÄ±ÅŸtÄ±r. YÃ¼ksek I/O gerektiren iÅŸlerin / dizininde Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± gerekmektedir.

Ä°ÅŸlerde bellek sÄ±nÄ±rlamasÄ± kullanÄ±lmaktadÄ±r. GÃ¶nderilen iÅŸlerin sunucularÄ±n bellek sÄ±nÄ±rlamalarÄ±na uygun olarak gÃ¶nderilmesi gerekmektedir. Bu kuyruk ile ilgili ayrÄ±ntÄ±lÄ± bilgi cuda-ui kullanÄ±cÄ± arayÃ¼zÃ¼ne baÄŸlandÄ±ktan sonra

.. code-block::

   scontrol show partition=kolyoz-cuda

komutu ile gÃ¶rÃ¼lebilir.